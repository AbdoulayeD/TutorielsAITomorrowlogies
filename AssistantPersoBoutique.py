
import os
from typing import Annotated, TypedDict, List
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage
from langchain_openai import ChatOpenAI
from langchain.agents import Tool
from langchain_community.tools import DuckDuckGoSearchRun
from langchain_community.document_loaders import UnstructuredExcelLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain_openai import OpenAIEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langgraph.graph import StateGraph, END
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda, Runnable, RunnableConfig
from langgraph.prebuilt import ToolNode, tools_condition
from langgraph.checkpoint.sqlite import SqliteSaver
from langchain.agents import tool
from langgraph.graph.message import AnyMessage, add_messages
import json
from langchain_community.tools.tavily_search import TavilySearchResults
from langchain_community.utilities import WikipediaAPIWrapper
import re
import streamlit as st
from whisper_stt import whisper_stt
from streamlit_extras.stylable_container import stylable_container
from langchain_core.messages import AIMessage
from openai import OpenAI
from pathlib import Path
import base64


import pandas as pd
import matplotlib.pyplot as plt
from langchain_core.tools import tool
# Assurez-vous d'avoir dÃ©fini votre clÃ© API OpenAI
import shutil
import uuid
from langchain.globals import set_debug
#set_debug(True)


# Assurez-vous d'avoir dÃ©fini votre clÃ© API OpenAI

openai_api_key="sk-XXXXXXXXX"

os.environ["OPENAI_API_KEY"] = openai_api_key


# DÃ©finir les types pour notre Ã©tat

    
class State(TypedDict):
    messages: Annotated[list[AnyMessage], add_messages]

# Fonction pour crÃ©er le systÃ¨me RAG pour la boutique
def create_store_qa():
    loader = UnstructuredExcelLoader("./boutique_database.xls")
    documents = loader.load()
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)
    texts = text_splitter.split_documents(documents)
    embeddings = OpenAIEmbeddings()
    vectorstore = Chroma.from_documents(texts, embeddings)
    return RetrievalQA.from_chain_type(
        llm=ChatOpenAI(),
        chain_type="stuff",
        retriever=vectorstore.as_retriever()
    )

# CrÃ©er les outils
search = DuckDuckGoSearchRun()
store_qa = create_store_qa()



@tool
def recherche_web(query: str) -> str:
    """
    Cet outil effectue une recherche web gÃ©nÃ©rale pour obtenir des informations sur divers sujets,
    y compris la mÃ©tÃ©o actuelle et les Ã©vÃ©nements rÃ©cents.

    Args:
    - query (str): La requÃªte de recherche.

    Returns:
    - str: Les rÃ©sultats de la recherche.

    Example:
    ```
    resultat = recherche_web("Quel temps fait-il aujourd'hui Ã  Paris ?")
    ```
    """
    return search.run(query)

@tool
def info_boutique(question: str) -> str:
    """
    Cet outil fournit des informations spÃ©cifiques sur la boutique, y compris les produits,
    les stocks, les ventes et d'autres donnÃ©es pertinentes.

    Args:
    - question (str): La question sur la boutique.

    Returns:
    - str: La rÃ©ponse Ã  la question sur la boutique.

    Example:
    ```
    reponse = info_boutique("Quels sont nos meilleurs produits ce mois-ci ?")
    ```
    """
    return store_qa.run(question)
    
    

    
    


@tool
def calculer_indicateurs_ventes():
    """
    Cet outil calcule plusieurs indicateurs de vente utiles pour la boutique en lisant les donnÃ©es depuis un fichier Excel.
    Il fournit des informations sur les ventes moyennes, les meilleures ventes, la croissance, etc.

    Returns:
    - str: Un rÃ©sumÃ© des indicateurs de vente calculÃ©s.

    Example:
    ```
    indicateurs = calculer_indicateurs_ventes()
    ```
    """
    # Charger les donnÃ©es depuis le fichier Excel
    df = pd.read_excel('boutique_database.xlsx')
    
    # Calculer la moyenne des ventes sur 6 mois pour chaque produit
    df['Moyenne 6 mois'] = df['Ventes (6 derniers mois)'].apply(lambda x: sum(map(int, x.split(', '))) / 6)
    
    # Calculer les indicateurs
    ventes_totales_dernier_mois = df['Ventes (dernier mois)'].sum()
    produit_plus_vendu = df.loc[df['Ventes (dernier mois)'].idxmax(), 'Produit']
    ventes_max = df['Ventes (dernier mois)'].max()
    produit_moins_vendu = df.loc[df['Ventes (dernier mois)'].idxmin(), 'Produit']
    ventes_min = df['Ventes (dernier mois)'].min()
    moyenne_ventes = df['Ventes (dernier mois)'].mean()
    
    # Calculer la croissance des ventes
    df['Ventes mois prÃ©cÃ©dent'] = df['Ventes (6 derniers mois)'].apply(lambda x: int(x.split(', ')[-2]))
    df['Croissance'] = (df['Ventes (dernier mois)'] - df['Ventes mois prÃ©cÃ©dent']) / df['Ventes mois prÃ©cÃ©dent'] * 100
    produit_plus_forte_croissance = df.loc[df['Croissance'].idxmax(), 'Produit']
    croissance_max = df['Croissance'].max()
    
    # Calculer la marge bÃ©nÃ©ficiaire moyenne
    df['Marge'] = (df['Prix de vente (â‚¬)'] - df['Prix d\'achat (â‚¬)']) / df['Prix de vente (â‚¬)'] * 100
    marge_moyenne = df['Marge'].mean()
    
    # PrÃ©parer le rÃ©sumÃ©
    resume = f"""
    Indicateurs de vente pour la boutique :
    
    1. Ventes totales du dernier mois : {ventes_totales_dernier_mois} unitÃ©s
    2. Produit le plus vendu : {produit_plus_vendu} avec {ventes_max} ventes
    3. Produit le moins vendu : {produit_moins_vendu} avec {ventes_min} ventes
    4. Moyenne des ventes par produit : {moyenne_ventes:.2f} unitÃ©s
    5. Produit avec la plus forte croissance : {produit_plus_forte_croissance} (+{croissance_max:.2f}%)
    6. Marge bÃ©nÃ©ficiaire moyenne : {marge_moyenne:.2f}%
    
    Conseil : Concentrez-vous sur la promotion de {produit_plus_vendu} tout en cherchant Ã  amÃ©liorer les ventes de {produit_moins_vendu}.
    Envisagez Ã©galement d'analyser le succÃ¨s de {produit_plus_forte_croissance} pour rÃ©pliquer cette croissance sur d'autres produits.
    """
    
    return resume


    
    

tools = [info_boutique,calculer_indicateurs_ventes, recherche_web]

# DÃ©finir le prompt principal de l'assistant
primary_assistant_prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "Vous Ãªtes un assistante IA nommÃ© Nova, conÃ§u pour gÃ©rer efficacement une boutique en utilisant les outils disponibles. "
            "Vous rÃ©pondez toujours poliment et votre fonction principale est d'interagir intelligemment et de traiter "
            "les informations avec prÃ©cision concernant les opÃ©rations de la boutique.\n\n"
            "Vous avez accÃ¨s aux outils suivants :\n{tools}\n\n"
            "Pour spÃ©cifier un outil, utilisez un blob JSON avec une clÃ© 'action' (nom de l'outil) et une clÃ© 'action_input' (entrÃ©e de l'outil).\n\n"
            "Valeurs 'action' valides : 'Final Answer' ou les noms des outils.\n\n"
            "Lorsque vous rÃ©pondez Ã  une question sur la boutique, suivez ce processus structurÃ© :\n\n"
            "1. Question : Question Ã  rÃ©pondre\n"
            "2. RÃ©flexion : DÃ©terminez si un outil est nÃ©cessaire pour rÃ©pondre Ã  la question. PrÃ©fÃ©rez toujours utiliser un outil pour les questions liÃ©es Ã  la boutique. "
            "En cas de doute, utilisez un outil. Ne spÃ©culez pas et ne fournissez pas d'informations au-delÃ  de la portÃ©e des outils.\n"
            "3. Action : Fournissez UNE SEULE action par blob JSON, comme indiquÃ© :\n\n"
            "```\n"
            "{{\n  \"action\": \"$NOM_OUTIL\",\n  \"action_input\": \"$ENTRÃ‰E\"\n}}\n"
            "```\n\n"
            "4. Observation : Enregistrez le rÃ©sultat de l'action, en vous assurant qu'il respecte le format de sortie spÃ©cifiÃ© par l'outil.\n"
            "5. VÃ©rification : VÃ©rifiez l'observation avec les informations connues et l'historique du chat. Si le rÃ©sultat n'est pas clair ou semble incorrect, indiquez que l'information n'a pas pu Ãªtre vÃ©rifiÃ©e.\n"
            "6. RÃ©ponse : Sur la base des informations vÃ©rifiÃ©es, formulez une rÃ©ponse. Assurez-vous que le format de la rÃ©ponse suit strictement le format de sortie spÃ©cifiÃ© par l'outil.\n\n"
            "RÃ©pÃ©tez le cycle RÃ©flexion/Action/Observation/VÃ©rification au maximum deux fois si nÃ©cessaire.\n\n"
            "Pour les questions non liÃ©es Ã  la boutique, rÃ©pondez directement sans utiliser d'outil uniquement si vous Ãªtes certain de la rÃ©ponse. En cas d'incertitude, indiquez que l'information n'est pas disponible.\n\n"
            "Action :\n"
            "```\n"
            "{{\n  \"action\": \"Final Answer\",\n  \"action_input\": \"Votre rÃ©ponse directe ici\"\n}}\n"
            "```\n\n"
            "Commencez ! N'oubliez pas de TOUJOURS rÃ©pondre avec un blob JSON valide d'une seule action. Utilisez des outils si nÃ©cessaire, vÃ©rifiez toutes les rÃ©ponses et tenez compte de l'historique du chat s'il peut aider Ã  rÃ©pondre Ã  la question.",
        ),
        ("placeholder", "{messages}"),
        ]
).partial(tools=tools)

# Fonction pour gÃ©rer les erreurs d'outils
def handle_tool_error(state) -> dict:
    error = state.get("error")
    tool_calls = state["messages"][-1].tool_calls
    return {
        "messages": [
            ToolMessage(
                content=f"Erreur : {repr(error)}\n veuillez corriger vos erreurs.",
                tool_call_id=tc["id"],
            )
            for tc in tool_calls
        ]
    }

# CrÃ©er un nÅ“ud d'outil avec gestion des erreurs
def create_tool_node_with_fallback(tools: list) -> dict:
    return ToolNode(tools).with_fallbacks(
        [RunnableLambda(handle_tool_error)], exception_key="error"
    )

# Initialiser le modÃ¨le
model = ChatOpenAI(model="gpt-4o-mini", temperature=0, api_key =openai_api_key, max_tokens=1024)

# CrÃ©er l'assistant
class Assistant:
    def __init__(self, runnable):
        self.runnable = runnable

    def __call__(self, state: State, config: RunnableConfig):
        while True:

            result = self.runnable.invoke(state)
            if not result.tool_calls and (
                not result.content
                or isinstance(result.content, list)
                and not result.content[0].get("text")
            ):
                messages = state["messages"] + [("user", "Respond with a real output.")]
                state = {**state, "messages": messages}
            else:
                break
        return {"messages": result}
    


primary_assistant_runnable = primary_assistant_prompt | model.bind_tools(tools)

# CrÃ©er le graphe
builder = StateGraph(State)

# DÃ©finir les nÅ“uds
builder.add_node("assistant", Assistant(primary_assistant_runnable))
builder.add_node("tools", create_tool_node_with_fallback(tools))

# DÃ©finir les arÃªtes
builder.set_entry_point("assistant")
builder.add_conditional_edges("assistant", tools_condition)
builder.add_edge("tools", "assistant")

graph = builder.compile()


# Compiler le graphe
def str_to_dict(json_str):
    # Supprimer les backticks et les espaces au dÃ©but et Ã  la fin
    json_str = json_str.strip().strip('`')
    
    try:
        # Convertir la chaÃ®ne JSON en dictionnaire
        result_dict = json.loads(json_str)
        return result_dict
    except json.JSONDecodeError as e:
        print(f"Erreur lors de la conversion JSON : {e}")
        return None
        
        




#Fonction pour interroger l'agent
def query_agent(question: str) -> str:
    config = {"configurable": {"thread_id": str(uuid.uuid4())}}
    result = graph.invoke({"messages": [HumanMessage(content=question)]}, config)
    result_content = result["messages"][-1].content
    match = re.search(r'"action_input":', result_content , re.DOTALL)
    if match:
        print("-->Found action input: ", result_content.find("action_input"))
        return result_content[result_content.rfind(":")+1: result_content.find("}")-1].replace('"', "")
    return result["messages"][-1].content.strip()

    
    
#print("----->", query_agent("Quel temps fait-il aujourd'hui Ã  Paris ?"))
#print("----->", query_agent("Quels sont nos meilleurs produits ce mois-ci ?"))
#print("----->", query_agent("Qui a gagnÃ© la Coupe du Monde de football en 2022 ?"))
#print("----->", query_agent("Donne moi le rÃ©sumÃ© des indicateurs de ventes "))





       

def autoplay_audio(file_path: str):
    with open(file_path, "rb") as f:
        data = f.read()
        b64 = base64.b64encode(data).decode()
        md = f"""
            <audio controls autoplay="true">
            <source src="data:audio/mp3;base64,{b64}" type="audio/mp3">
            </audio>
            """
        st.markdown(
            md,
            unsafe_allow_html=True,
        )


speech_file_path = Path(__file__).parent / "speech.mp3"



client = OpenAI(api_key=openai_api_key)

st.title("ðŸ’¬ Assistant Personnel ")

if "messages" not in st.session_state:
    st.session_state["messages"] = [{"role": "assistant", "content": "Bonjour Abdou ðŸ™‚"}]

for msg in st.session_state.messages:
    st.chat_message(msg["role"]).write(msg["content"])
with stylable_container(
        key="bottom_content",
        css_styles="""
            {
                position: fixed;
                bottom: 105px;
            }
            """,
    ):audio = whisper_stt(openai_api_key=openai_api_key, language = 'fr')
       
       
if prompt := st.chat_input("Ask here"):

    st.session_state.messages.append({"role": "user", "content": prompt})

    st.chat_message("user").write(prompt)
    message = st.session_state.messages
    try:
  
        response = query_agent(str(prompt ))
        response_str = str(response)
        if response_str == "":
            response_str = "You're request didn't succeed. Can You ask the question again ?"
        st.session_state.messages.append({"role": "assistant", "content": response_str})
        
        
        
        audio_response = client.audio.speech.create(
          model="tts-1",
          voice="nova",
          input=response_str
        )
        audio_response.stream_to_file(speech_file_path)
        
        
        st.chat_message("assistant").write(response_str[:-1])
        autoplay_audio("./speech.mp3")

    #except err:
    except Exception as e:
        st.error( f"The agent_excecutor catch an error {e}, please try again!", icon="ðŸš¨")
        #return f"An error occurred: {e}"
    
        # st.error('The agent_excecutor catch an error, please try again!', icon="ðŸš¨")
        response_str = "The agent_excecutor catch an error, please try again!"
        st.session_state.messages.append({"role": "assistant", "content": response_str})

if audio:
    st.session_state.messages.append({"role": "user", "content": audio})
    st.chat_message("user").write(audio)

    message = st.session_state.messages
    try:

        response = query_agent(str(audio))


        response_str = str(response)
        if response_str == "":
            response_str = "You're request didn't succeed. Can You ask the question again ?"
        st.session_state.messages.append({"role": "assistant", "content": response_str})
        
        
        audio_response = client.audio.speech.create(
          model="tts-1",
          voice="nova",
          input=response_str
        )
        audio_response.stream_to_file(speech_file_path)

        st.chat_message("assistant").write(response_str)
        autoplay_audio("./speech.mp3")

    except:
        st.error('The agent_excecutor catch an error, please try again!', icon="ðŸš¨")
        response_str = "The agent_excecutor catch an error, please try again!"
        st.session_state.messages.append({"role": "assistant", "content": response_str})
